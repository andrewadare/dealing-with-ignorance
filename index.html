<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><title>PhET Talk</title><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui"><link rel="stylesheet" href="reveal.js/css/reveal.css"><link rel="stylesheet" href="css/black.css"><link rel="stylesheet" href="reveal.js/lib/css/zenburn.css"></head><body><div class="reveal"><div class="slides"><section data-background="figs/gammas.svg"><h1>Dealing With Ignorance</h1><p style="text-align: center;"><span class="red"><strong>Learning and Probabilistic Reasoning</strong></span></p><p style="text-align: center;">Andrew M. Adare</p><p style="text-align: center;">January 26, 2016</p><aside class="notes"><ul><li>asked to talk about an interest</li><li>play to strengths - talk about ignorance</li></ul></aside></section><section class="center"><q style="text-align: center;">When the facts change, I change my opinion.</q><br><q style="text-align: center;">What do you do, sir?</q><br><p style="text-align: right">John Maynard Keynes</p><aside class="notes"><p>But when confronted with new data, I update my beliefs.</p></aside></section><section><p><span class="green"><strong>knowlÂ·edge</strong></span></p><ol><li>facts, information, and skills acquired by a person through experience or education; the theoretical or practical understanding of a subject.</li><li>awareness or familiarity gained by experience of a fact or situation.</li></ol><br><p class="fragment">This talk is about the transition from <span class="blue">belief</span> to <span class="green">knowledge</span> through <span class="red">evidence</span> and probabilistic reasoning.</p><aside class="notes"><ul><li>belief, knowledge- loaded words</li><li>but they'll take specific meanings</li><li>Not about epistemology or cognition</li><li>Interested in how predictive models learn</li><li>in turn gives us learning and insight</li></ul></aside></section><section><h2>Bayesian Inference</h2><p><span class="green">Fascinating history:</span></p><p>Developed for apologetics; applied in war; suppressed and discredited for centuries; now rocking the mic</p><p data-fragment-index="1" class="fragment"><span class="green">Essential features:</span><ul data-fragment-index="1" class="fragment"><li>Probability as subjective belief</li><li>A priori information important--and explicit</li><li>Model parameters as random variates</li></ul></p><br><p data-fragment-index="2" class="fragment">Contrast this with frequentist inference:</p><aside class="notes"><ul><li>Thomas Bayes, LaPlace, Turing</li><li>Fischer, Neyman, Pearson</li><li>Resurgence</li></ul></aside></section><section><h2>Frequentist statistics</h2><p>Any experiment is viewed as one of an infinite sequence of similiar, independent experiments</p><p class="fragment">Key technique: Method of Maximum Likelihood</p><p class="fragment">Unknown parameters typically treated as fixed values; data points get error bars</p><p class="fragment">Unnatural/problematic for rare events like earthquakes, stock market crashes</p><aside class="notes"><ul><li>Characterized by hypothesis tests, confidence intervals</li><li>p-values banned from some journals</li><li>Nate Silver video</li></ul></aside></section><section><h2>Probabilities: the big three</h2><div class="lefthalf"><p>Consider this 2D histogram:</p><img data-src="figs/prob-grid.png" width="100%"><p>N is the sum of all cells</p></div><div class="righthalf fragment"><p><span class="green">Joint $$ p(x_i, y_j) = \frac{n_{ij}}{N} $$</span></p><p><span class="yellow">Marginal $$ p(x_i) = \frac{c_i}{N} $$</span></p><p><span class="red">Conditional $$ p(y_j\,|\,x_i) = \frac{n_{ij}}{c_i} $$</span></p></div></section><section><h2>Product rule of probability</h2><div class="lefthalf"><p><span class="green">Joint $$ p(x_i, y_j) = \frac{n_{ij}}{N} $$</span></p><p><span class="yellow">Marginal $$ p(x_i) = \frac{c_i}{N} $$</span></p><p><span class="red">Conditional $$ p(y_j\,|\,x_i) = \frac{n_{ij}}{c_i} $$</span></p></div><div class="righthalf fragment"><p>Combine:</p><p><span class="green">$ p(x_i, y_j) = $</span>  <span class="red">$ \frac{n_{ij}}{c_i}$ </span> <span class="yellow">$\frac{c_i}{N} $</span></p><p><span class="green">$ = $</span> <span class="red">$ p(y_j\,|\,x_i)$</span> <span class="yellow">$ p(x_i) $</span></p></div></section><section><h2>Bayes' Rule</h2><p>$$ p(x, y) = p(y\,|\,x)\,p(x) = p(x\,|\,y)\,p(y) $$</p><br><p>$$ p(y\,|\,x) = \frac{p(x\,|\,y)\,p(y)}{p(x)} $$</p><br><p>$$ \mathrm{posterior} = \frac{\mathrm{likelihood} \times \mathrm{prior}}{\mathrm{evidence}} $$</p></section><section><h2>Statistical learning</h2><p>Two goals, given a dataset $ \{ \mathbf{x}_{i} \}_{i=1}^N $</p><ol><li><span class="green">Learn:</span> find the unknown parameters $\mathbf{\theta}$ of a model by fitting to the data.</li><li class="fragment"><span class="green">Predict:</span> calculate the probability of a new datum $\mathbf{x}^*$ under the model</li></ol><div class="fragment"><p>Three general approaches</p><ol><li>Maximum Likelihood (ML) Estimation</li><li>Maximum A Posteriori (MAP) estimation</li><li>Fully Bayesian methods</li></ol></div><aside class="notes"><ul><li>Learning means generalization</li><li>A good predictive also incredibly powerful + $, hence data sci</li></ul></aside></section><section><h2>Maximum Likelihood Estimation</h2><p>Find the set of parameters under which the data are most likely: 
$$
\hat{\mathbf{\theta}} 
= \underset{\mathbf{\theta}}{\mathrm{argmax}}
\left[ p(\mathbf{x}_{1\ldots I} \,|\, \mathbf{\theta}) \right] 
= \underset{\mathbf{\theta}}{\mathrm{argmax}} \left[ 
\prod_{i=1}^I p(\mathbf{x}_i \,|\, \mathbf{\theta})
\right]
$$</p><div class="lefthalf"><p>Ordinary Least Squares works as an ML estimator for Gaussian likelihood (by minimizing $-\!\ln p(\mathbf{x} \,|\, \mathbf{\theta})$)</p></div><div class="righthalf"><img data-src="figs/gas-prices.png" width="100%"></div></section><section><h2>Maximum A Posteriori</h2><p>MAP estimation uses Bayes' rule (without the denominator)
$$
\hat{\mathbf{\theta}} 
= \underset{\mathbf{\theta}}{\mathrm{argmax}}
\left[ 
p(\mathbf{\theta} \,|\, \mathbf{x}_{1\ldots I})
\right] 
= \underset{\mathbf{\theta}}{\mathrm{argmax}} \left[ 
\prod_{i=1}^I p(\mathbf{x}_i \,|\, \mathbf{\theta})
p(\mathbf{\theta})
\right]
$$
This generalizes ML estimation by introducing prior knowledge/belief via $ p(\mathbf{\theta}) $.</p><aside class="notes">So MLE is just a special case of MAP estimation, when the prior is uninformative.

</aside></section><section><h2>Fully Bayesian Approach</h2><p>Many choices of $\mathbf{\theta}$ may be consistent with the data. </p><p class="fragment">Point estimates cannot accommodate that.</p><p class="fragment">Fully Bayesian methods calculate the full joint posterior probability over the model parameters:
$$
p(\mathbf{\theta} \,|\, \mathbf{x}_{1\ldots I}) = \frac{\prod_{i=1}^I p(\mathbf{x}_i \,|\, \mathbf{\theta}) p(\mathbf{\theta})} {p(\mathbf{x}_{1\ldots I})} $$</p><p class="fragment">This is a distribution over possible models.</p><aside class="notes"><ul> </ul><li>Accept reality: there may be many choices of theta that are consistent with the data.</li><li>Point estimates sweep that under the rug.</li><li>Discuss dimensionality of posterior</li></aside></section><section><h2>Predictive distributions</h2><p>$p(\mathbf{x}^* \,|\, \mathbf{\theta})$ gives us a prediction for the unseen data $\mathbf{x}^*$ for a given $\mathbf{\theta}$.</p><p class="fragment">Since there are many possible $\mathbf{\theta}$ values, we must integrate over them all, weighting by their probability:
$$
p(\mathbf{x}^* \,|\, \mathbf{x}_{1\ldots I}) = 
\int p(\mathbf{x}^* \,|\, \mathbf{\theta}) \, p(\mathbf{\theta} \,|\, \mathbf{x}_{1\ldots I}) d\mathbf{\theta}
$$</p><p class="fragment"><span class="green">A unified picture:</span> "posterior" from ML and MAP estimation is like a $\delta$-function at $\mathbf{\hat\theta}$.</p><p class="fragment">So predicting $\mathbf{x}^*$ simply amounts to evaluating $p(\mathbf{x}^* \,|\, \mathbf{\hat\theta })$.</p></section><section> <h2>A minimal example: coin flipping</h2><p>How do we test whether a coin is fair?</p><p>Use the <span class="red">binomial distribution</span> to model the likelihood:
$$
p(k\,|\, N, \theta) = \binom{N}{k} \theta^k (1-\theta)^{N-k}
$$
$\theta$ controls the probability to obtain $k$ heads in $N$ flips.</p><aside class="notes"><ul><li>Classic inverse problem:</li><li>Infer cause (th) from effect (k)</li></ul></aside>
</section><section><h2>The Beta-Binomial Model</h2><p>Use a <span class="green">Beta distribution</span> for the prior $p(\theta)$:
$$ \mathrm{Beta}(\theta \,|\, a, b) \propto
\theta^{a - 1}(1 - \theta)^{b - 1} $$</p><div class="lefthalf"><p><span class="red">Conjugate prior</span> to binomial likelihood</p><p><span class="yellow">Uninformative</span> if $a = b = 1$ (uniform distribution)</p><p><span class="blue">Posterior:</span> $\mathrm{Beta}(\theta \,|\, N_h + a, N_t + b)$</p></div><div class="righthalf"><img data-src="figs/Beta_distribution_pdf.svg" style="background-color: white;" width="100%"></div></section><section><h2>Coin-Flip Simulation</h2><p>Max. Likelihood vs Bayesian shootout</p><ul><li>Generate coin tosses using a fair coin ($\theta = 0.5$)</li><li>Compute estimators for $\theta$ from the outcomes</li></ul><br><p>We will compare $$\hat{\theta}_{MLE} = \frac{N_{h}}{N}$$ </p><p>vs. the posterior mean $$E[\theta \,|\, \mathrm{outcomes}]$$</p></section><section><video controls class="stretch"><source data-src="flip.mp4" type="video/mp4"><Your>browser doesn't support HTML5 video in MP4 with H.264.</Your></video></section><section><h2>Observations</h2><p>Both methods agree on the truth as $N \to \infty$</p><p class="fragment">After two flips (both heads up), the MLE mean was $\frac{N_{h}}{N} = 1$ with a variance of zero!</p><p class="fragment"><span class="red">Overfitting:</span> that prediction wouldn't generalize.</p><p class="fragment">But the Bayesian estimate is immediately closer to the correct answer, with a saner uncertainty</p><p class="fragment">Posterior mean $ = \frac{N_{h} + a}{N + a + b} = \frac{3}{4} $
with a variance of $0.19^2$.</p><aside class="notes">See Murphy 3.3 for details

</aside></section><section><h2>SLAM</h2><p class="small">Simultaneous Localization and Mapping</p><div style="display: inline-block; float:left; width:56%;"><img data-src="figs/slam-diagram.png"></div><div style="display: inline-block; float:right; width:40%;"><p>Start at unknown location in an unknown environment</p><p>Incrementally build a consistent map</p><p>Simultaneously determine location within map</p></div></section><section><h2>Recursive state estimation</h2><p>SLAM can be solved using an extended Kalman filter (EKF)</p><p>The EKF is a specialization of Bayesian filtering for linearized Gaussian models</p><br><p>Simulation: autonomous navigation using predefined waypoints, landmarks observed with a rangefinder, and Bayesian filtering</p></section><section><video controls class="stretch"><source data-src="ekf-slam.mp4" type="video/mp4"><Your>browser doesn't support HTML5 video in MP4 with H.264.</Your></video></section><section><img data-src="figs/autonomous-car.png" class="stretch"></section><section><h2>Summary</h2><ul><li>Statistical learning involves initial beliefs, experience, and a conformant model</li><li class="fragment">Much like human learning!</li><li class="fragment">Machine learning is way too cool to be co-opted by marketers and advertisers</li></ul></section><section class="center"><h3>Extras</h3></section><section><h2>Gaussian models        </h2><p>The Gaussian is the highest-entropy distribution</p><img data-src="figs/gaussian.png" width="70%"></section><section><h2>Linear regression demo: a Bayesian rendition</h2><p>Outline of demo, skipping many details:</p><ul><li>Synthetic data is generated from $f(x,\mathbf{a}) = a_0 + a_1 x$</li><li>Goal: infer $a_0$ and $a_1$ from a fit to limited data</li><li>Noise is added to $f(x_n)$ to give target values $t_n$</li><li>Likelihood: $p(t|x,w,\beta) = \prod_n N(t_n|w^T \phi, 1/\beta)$</li><li>Gaussian prior on $w_0$, $w_1$: $p(w|\alpha) = N(0, \Sigma)$, where $\Sigma = 1/\alpha \, I$</li></ul><p>The posterior is thus also Gaussian: $p(w|t) = N(w|m_N, S_N)$, where $m_N = \beta S_N \Phi^T t$, and $S_N^{-1} = \alpha I + \beta \Phi^T \Phi$

</p></section><section><h2>Hidden function and prior</h2><img data-src="svg/true_y.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/prior.svg" style="background-color: white;" class="righthalf"></section><section><section><img data-src="svg/post01.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred01.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post02.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred02.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post03.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred03.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post04.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred04.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post05.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred05.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post06.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred06.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post07.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred07.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post08.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred08.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post09.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred09.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post10.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred10.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post11.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred11.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post12.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred12.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post13.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred13.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post14.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred14.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post15.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred15.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post16.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred16.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post17.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred17.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post18.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred18.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post19.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred19.svg" style="background-color: white;" class="righthalf"></section><section><img data-src="svg/post20.svg" style="background-color: white;" class="lefthalf"><img data-src="svg/pred20.svg" style="background-color: white;" class="righthalf"></section></section></div></div><script src="reveal.js/lib/js/head.min.js"></script><script src="reveal.js/js/reveal.js"></script><script src="init.js"></script></body></html>